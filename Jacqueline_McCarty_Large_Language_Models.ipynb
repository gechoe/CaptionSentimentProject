{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamccarty/CaptionSentimentProject/blob/main/Jacqueline_McCarty_Large_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring GPT-3\n",
        "\n",
        "In this homework assignment we will walk you through how to use GPT-3 a large pre-trained neural language model developed by OpenAI.  \n",
        "\n",
        "You will learn about the following topics:\n",
        "* Prompts and completions.  You should observe that the the quality of the text generated is high quality, but not necessarially factually accurate.\n",
        "* Probabilities.  You'll learn how to inspect probabilities assigned to words in the model's output.\n",
        "* Few shot learning.  We'll see an example of few-shot learning with a small handful of examples.\n",
        "* Zero shot learning.  We will explore the zero-shot capabilities of pre-trained LMs.  You'll design zero-shot prompts for\n",
        "1. summarization\n",
        "2. question-answering\n",
        "3. simplification\n",
        "4. translation\n",
        "* How to fine tune a model.  You will learn how to fine-tune GPT-3 to take a Wikipedia infobox as input and generate the text of a biography as its ouput.  You'll then write your own code to do the reverse task – given a biography, extract the  attributes and values in the style of a Wikipedia infobox.\n",
        "\n",
        "\n",
        "\n",
        "# Prompt Completion\n",
        "\n",
        "As a warm-up we'll have you play with [the OpenAI Playground](https://beta.openai.com/playground).  Try inputting this prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is\n",
        "\n",
        "And the click the \"Submit\" button to generate a completion.\n",
        "\n",
        "Copy and paste the text below (including your prompt).\n",
        "\n",
        "You might notice that the text that GPT-3 generates ends mid-sentence.  GPT-3 will generate text until it either generates a special \"stop sequence\" token `<|endoftext|>`, or it outputs the number of tokens specified by the `maximum length` variable.\n",
        "You can press Submit again to have it continue generatin, or you can increase the max length variable in the sliderbar on the right."
      ],
      "metadata": {
        "id": "pjqy7heO706G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_1 = \"\"\"\n",
        "My favorite professor at the University of Pennsylvania is Dr. Charlotte Emerson. I have had the pleasure of attending her lectures on a variety of topics, including history, literature, and philosophy. She is an incredibly engaging speaker who is able to make complex ideas understandable to the average student. I highly recommend her as a professor to anyone looking to gain a better understanding of the world around them!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZwhSf4Af79Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3 generates fluent text, but it is not always grounded in fact.  Let's do a Google search for the person that GPT-3 generated as your favorite professor and check\n",
        "* Are they actually a professor?\n",
        "* Where do they work?"
      ],
      "metadata": {
        "id": "nhYnYplw8K4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the professor's name\n",
        "professor_name_1 = \"Charlotte Emerson\"\n",
        "\n",
        "# Do a Google search and answer these questions\n",
        "actually_a_professor_1 = False\n",
        "\n",
        "# Insitituion where they work\n",
        "instituion_1 = \"University of Florida\""
      ],
      "metadata": {
        "id": "X8-RRIm8-W_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it generates its completions, GPT-3 generates each new word/token according to its probability distribution.  It draws each word at random in proportion to its propability.  That randomness means that it can generate different completions. You can re-generate and get different completions each time.\n",
        "\n",
        "Generate another 4 completions for the professor prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is\n",
        "\n",
        "and do Google searches for them.\n",
        "\n",
        "*Tip: You can generate another response with the Regenerate button to the right of the Submit button.  The Regenerate button has a recycle symbol on it.*"
      ],
      "metadata": {
        "id": "37sJsYna_jRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_2 = \"\"\"\n",
        "My favorite professor at the University of Pennsylvania is Chancellorxs1. He is alwaysorganized and on top of his material. He is always willing to help out students and is always willing to engage in discussion.\n",
        "\n",
        "Another great professor at the University of Pennsylvania is Dr. Carey Russ. He is always able to engage students in interesting discussions and is always willing to help out students.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_3 = \"\"\"\n",
        "My favorite professor at the University of Pennsylvania is Edward Said. I absolutely adore his lectures and readings, and can always learn something new from him. He has a very unique perspective on literature and culture, and I really enjoy hearing about his experiences as a foreign student and critic.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_4 = \"\"\"\n",
        "My favorite professor at the University of Pennsylvania is R. Stephen Montgomery. He is an incredible teacher, and his insights into literature and history are second to none. He has a great sense of humor, and he makes his classes both engaging and informative. I would highly recommend him to anyone looking for a gifted teacher who can help them develop their creative writing skills.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_5 = \"\"\"\n",
        "My favorite professor at the University of Pennsylvania is Allen Buchanan. I have had the opportunity to work with him in a coursework setting and in a research setting. I appreciate his willingness to share his knowledge, his expertise, and his experiences with his students. He is a excellent teacher and a exemplary scholar.\n",
        "\n",
        "I have had a wonderful experience studying under Professor Buchanan. He is an outstanding educator who is passionately committed to providing his students with excellent instruction. He is always willing to work collaboratively with his students, and he is able to mobilize a wide range of resources to support their learning. He is also a very well- researched scholar, and his knowledge of the literature is constantly expanding. I have enjoyed participating in his research projects, and I have learned a great deal from his insights and experience. I would highly recommend Professor Buchanan as an instructor and a researcher.My favorite professor at the University of Pennsylvania is Allen Buchanan. I have had the opportunity to work with him in a coursework setting and in a research setting. I appreciate his willingness to share his knowledge, his expertise, and his experiences with his students. He is a excellent teacher and a exemplary scholar.\n",
        "\n",
        "I have had a wonderful experience studying under Professor Buchanan. He is an outstanding educator who is passionately committed to providing his students with excellent instruction. He is always willing to work collaboratively with his students, and he is able to mobilize a wide range of resources to support their learning. He is also a very well- researched scholar, and his knowledge of the literature is constantly expanding. I have enjoyed participating in his research projects, and I have learned a great deal from his insights and experience. I would highly recommend Professor Buchanan as an instructor and a researcher.\n",
        "\"\"\"\n",
        "\n",
        "# Do a Google search for these professors\n",
        "\n",
        "professor_name_2 = \"Chancellorxs1\"\n",
        "actually_a_professor_2 = False\n",
        "instituion_2 = \"Unclear\"\n",
        "\n",
        "professor_name_3 = \"Edward Said\"\n",
        "actually_a_professor_3 = False\n",
        "instituion_3 = \"Columbia University\"\n",
        "\n",
        "professor_name_4 = \"R. Stephen Montgomery\"\n",
        "actually_a_professor_4 = False\n",
        "instituion_4 = \"University of Bristol\"\n",
        "\n",
        "professor_name_5 = \"Allen Buchanan\"\n",
        "actually_a_professor_5 = False\n",
        "instituion_5 = \"University of Arizona\""
      ],
      "metadata": {
        "id": "zex_KrxrAVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilities\n",
        "\n",
        "Just like with the n-gram language models that we stuided earlier in the course, neural language models like GPT-3 assign probabilities to each token in a sequence.  \n",
        "\n",
        "In the playground, you can see the probabilities for the top-5 words predicted at each position by choosing the `Full Spectrum` option from the `Show probabilities` dropdown menu in the controls.  Try selecting that option and then generate a completion for the prompt\n",
        "\n",
        "> My favorite class in the Computer Science Department was taught by Professor\n",
        "\n",
        "If you mouse over the word after professor, you'll see something like this:\n",
        "```\n",
        "Joe = 8.21%\n",
        "John = 4.25%\n",
        "Nancy = 2.27%\n",
        "David = 2.09%\n",
        "Barbara = 2.05%\n",
        "Total: -2.50 logprob on 1 tokens\n",
        "(18.87% probability covered in top 5 logits\n",
        "```\n",
        "\n",
        "One critical observation about language models is that they often encode societal biases that appear in their data.  For instance, after the disovery that LM embeddings could be used to solve word analogy problems like \"**man** is to **woman** as **king** is to ___\" (the model predicts **queen**), researchers discovered that LMs had a surpisingly sexist answer to the analogy problem  \"**man** is to **woman** as **computer programmer** is to ___\" (the model predicts **homemaker**).  These kinds of biases are prevelant and pernicious.\n",
        "\n",
        "Let's examine the most probable names that GPT3 assigns to different completions and analyze their gender.  We'll see if it associates different genders with different academic disciplines.  (You can also see this for different careers like *nurse*, *plumber*, or *school teacher*).\n",
        "\n",
        "Please create dictionaries mapping GPT's predictions for the first names of professors in these departmemnts\n",
        "* Computer Science\n",
        "* Gender Studies\n",
        "* Physics\n",
        "* Linguisticss\n",
        "* Bioengineering\n",
        "Use the prompt:\n",
        "> My favorite class in the {deparment_name} Department was taught by Professor\n",
        "\n",
        "**Note: you can also add a stop sequence of `.` to get the model to complete only a single sentence.**\n",
        "\n"
      ],
      "metadata": {
        "id": "y0bIhdi24moc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classify each name as male, female, partial word, or unknown\n",
        "computer_science_genders = {\n",
        "  \"David\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"Michael\" : \"male\",\n",
        "  \"S\" : \"partial word\",\n",
        "  \"R\" : \"partial word\",\n",
        "  \"Gerald\" : \"male\"\n",
        "}\n",
        "\n",
        "gender_studies_genders = {\n",
        "  \"K\" : \"partial word\",\n",
        "  \"Lisa\" : \"female\",\n",
        "  \"L\" : \"partial word\",\n",
        "  \"Laura\" : \"female\",\n",
        "  \"Judith\" : \"female\",\n",
        "  \"Carolina\" : \"female\"\n",
        "}\n",
        "\n",
        "physics_genders = {\n",
        "  \"John\" : \"male\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Emer\" : \"female\",\n",
        "  \"K\" : \"partial word\",\n",
        "  \"P\" : \"partial word\",\n",
        "  \"Mark\" : \"male\"\n",
        "}\n",
        "\n",
        "lingusitics_genders = {\n",
        "  \"John\" : \"male\",\n",
        "  \"Barbara\" : \"female\",\n",
        "  \"Debra\" : \"female\",\n",
        "  \"K\" : \"partial word\",\n",
        "  \"James\" : \"male\",\n",
        "  \"Susan\" : \"female\"\n",
        "}\n",
        "\n",
        "bioengineering_genders = {\n",
        "  \"John\" : \"male\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Jian\" : \"male\",\n",
        "  \"K\" : \"partial word\",\n",
        "  \"S\" : \"partial word\",\n",
        "  \"Reid\" : \"male\"\n",
        "}"
      ],
      "metadata": {
        "id": "rj2Dt1BL4l-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(If you wanted to systematically explore the predictions of the model, you could use the API's logprobs argument to return the the log probabilities on the logprobs most likely tokens, as well the chosen tokens.)"
      ],
      "metadata": {
        "id": "DckHlrauFYdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Learning\n",
        "\n",
        "One of the remarkable properties of large language models is a consequence of the fact that they have been trained on so much language data.  They encode that training data as background information that lets them learn new tasks and to generalize patterns using only a few examples.  This is called \"Few shot learning\".\n",
        "\n",
        "Here is an example.  Imagine that we want to build a system that allows a student to say something they want to learn, and the system will recommend the subject for them to study.  Here are examples of inputs and outputs to our program:\n",
        "\n",
        "```\n",
        "how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "```\n",
        "\n",
        "We can use these 7 examples (and probably fewer!) as a prompt to GPT-3, and it will perform few shot learning by figuring out what our pattern is, and being able to perform the task for new inputs.\n",
        "\n",
        "Try pasting those examples into the Playground, and then listing out a few subjects to see what is output.\n",
        "\n",
        "```\n",
        "cellular respiration\n",
        "respiratory disease\n",
        "how to play saxophone\n",
        "autonomic system\n",
        "how write a screenplay\n",
        "perform in a play\n",
        "stock market\n",
        "planetary orbits\n",
        "relativity\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qV65mL6va3lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the dictionary below using the playground by replacing the TODOs with the model's predictions."
      ],
      "metadata": {
        "id": "CRMLTBoxe6V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_subject_classification_results = {\n",
        "  \"cellular respiration\" : \"biology\",\n",
        "  \"respiratory disease\" : \"medical\",\n",
        "  \"how to play saxophone\" : \"music\",\n",
        "  \"autonomic system\" : \"overview\",\n",
        "  \"how write a screenplay\" : \"physiology\",\n",
        "  \"perform in a play\" : \"theater\",\n",
        "  \"stock market\" : \"finance\",\n",
        "  \"planetary orbits\" : \"astronomy\",\n",
        "  \"relativity\" : \"theory - physics\",\n",
        "}"
      ],
      "metadata": {
        "id": "cTaSq85Ka8WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the API\n",
        "\n",
        "Now let's take a look at how to call the OpenAI API from our code, so that we don't have to manually enter inputs into the Playground.  \n",
        "\n",
        "If you click on the \"View code\" button on the playground, you'll see a sample of code for whatever prompt you have.  For example, here's the code that we have for our few-shot learning that generates a subject to study for a topic that someone is interested in:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-002\",\n",
        "  prompt=\"how to program in Python - computer science\\nfactors leading up to WW2 - history\\nbranches of government - political science\\nShakespeare's plays - English\\ncellular respiration - biology\\nrespiratory disease - medical\\nhow to sculpt - art\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "```\n",
        "This is python code, so it'll be pretty easy for us to use this as a starting point and to modify it to create a function that we can call.\n",
        "\n",
        "\n",
        "First, you'll need install the OpenAPI via pip.  You can use pip and other Unix command in a colab notebook by prefixing them with an exclamation point.  (The `%%capture` command before that just surpresses the output of running the Unix command.  You can remove it if you want to see the progress of the command).\n"
      ],
      "metadata": {
        "id": "4VpROz_FfJZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "396iGnE4ra9g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will enter your secret key for the OpenAI API, then you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys).  \n",
        "\n",
        "We will enter it as a password, so that the raw text of it doesn't get saved in your Python notebook and you accidentally make your notebook public.  That would be bad because then other people could use your key and have you pay for their usage."
      ],
      "metadata": {
        "id": "9jdqGfOyrmhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import openai\n",
        "import os\n",
        "\n",
        "print('Enter OpenAI API key:')\n",
        "openai.api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PIBX87qrlDd",
        "outputId": "767d5bd6-af0b-44b8-bd68-e676c2143288"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's write a function that takes a topic as input and then outputs a subject to study if you want to learn about that topic."
      ],
      "metadata": {
        "id": "yt7VVKVtvcCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "def generate_subject_few_shot(topic):\n",
        "  few_shot_prompt = \"\"\"how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "\"\"\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=few_shot_prompt + topic + \" - \", # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call,\n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains\n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion.\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "topic = \"cellular respiration\"\n",
        "generate_subject_few_shot(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o9-oV9mnvmtD",
        "outputId": "30598aef-f665-4dc2-e6a7-487fc6a403d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'biology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it!  That's an exampe of how to write a function call to the OpenAI API in order for it to output a subject for a topic.\n",
        "\n",
        "Here is some information about the different arguments that we to the `openai.Completion.create` call:\n",
        " * `model` – OpenAI offers four different sized versionf of the GPT-3 model: davinci, currie, babbage and ada.  Davinci has the largest number of parameters and is [the most expensive to run](https://openai.com/api/pricing/).  Ada has the fewest parameters, is the fastest to run and is the least expensive.\n",
        " * `prompt` - this is the prompt that the model will generate a completion for\n",
        " * `temperature` - controls how much of the probability distribution the model will use when it is generating each token. 1.0 means that it samples from the complete probability distrubiton, 0.7 means that it drops the bottom 30% of the least likely tokens when it is sampling. 0.0 means that it will perform deterministically and always output the single most probable token for each context.\n",
        " * `top_p` - is an alternative way of controling the sampling.\n",
        " * `frequency_penalty` and `presence_penalty` are two ways of reduing the model from repeating the same words in one output.  You can set these to be >0 if you're seeing a lot of repetition in your output.\n",
        " * `max_tokens` is the maximum length in tokens that will be output by calling the function.  A token is a subword unit.  There are roughly 2 or 3 tokens per word on average.\n",
        " * `stop` is a list of stop sequences.  The model will stop generating output once it generates one of these strings, even if it hasn't reached the max token length. By default this is set to a special token `<|endoftext|>`.\n",
        "\n",
        "You can read more about [the Completion API call in the documentation](https://beta.openai.com/docs/api-reference/completions)."
      ],
      "metadata": {
        "id": "Wmr06VEzxlnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero shot learning\n",
        "\n",
        "In addition to few shot learning, GPT-3 can sometimes also perform \"zero shot learning\" where instead of giving it several examples of what we want it to do, we can instead give it instructions of what we want it to do.\n",
        "\n",
        "For example, for our topic - subject task we could give GPT-3 the prompt\n",
        "\n",
        "> Given a topic, output the subject that a student should study if they want to know more about that topic.\n",
        "\n",
        "Then if we append\n",
        "> cellular respiration -\n",
        "\n",
        "GPT3 will output biology.\n",
        "\n",
        "Try to adapt the `generate_subject_few_shot` function to do a zero-shot version."
      ],
      "metadata": {
        "id": "h5iKKme91RMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_zero_shot(prompt, link):\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=prompt + \" \" + link, # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "  )\n",
        "\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains\n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion.\n",
        "  return response['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "aLve17Cl3d2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very cool recent finding is that training proceedure for large language models can be changed to improve this instruction following behavior.  If large LMs are [trained to do multiple tasks through prompting](https://arxiv.org/abs/2110.08207), they better generalize to complete new tasks in a zero-shot fashion.  The current version of GPT3 (text-davinci-2) uses this kind of training.\n",
        "\n",
        "Try writing zero-shot prompts to do the following tasks:\n",
        "1. Summarize a Wikipedia article.\n",
        "2. Answer questions about an article.\n",
        "3. Re-write an article so that it's suitable for a young child who is just learning how to read (age 8 or so).\n",
        "4. Translate an article from Russian into English.\n",
        "\n",
        "You should experiment with a few prompts in the playground to find a good prompt that seems to work well."
      ],
      "metadata": {
        "id": "O3eB1xva5Nr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(article_text):\n",
        "  # TODO - write this function\n",
        "  prompt = \"Summarize this article:\"\n",
        "  summary = generate_subject_zero_shot(prompt, article_text)\n",
        "  return summary\n",
        "\n",
        "def answer_question(article_text, question):\n",
        "  # TODO - write this function\n",
        "  answer = generate_subject_zero_shot(article_text, question)\n",
        "  return answer\n",
        "\n",
        "def simplify(article_text):\n",
        "  # TODO - write a function to re-write an article so that it's suitable for a young child.\n",
        "  prompt = \"Rewrite this article so it would be suitable for an eight-year-old child. \"\n",
        "  simplified_article = generate_subject_zero_shot(article_text, prompt)\n",
        "  return simplified_article\n",
        "\n",
        "def translate(article_text, source_language, target_language):\n",
        "    # TODO - write a function to translate an article from a source language to a target language.\n",
        "  prompt = \"Translate this article from \" + source_language + \" to \" + target_language\n",
        "  simplified_article = generate_subject_zero_shot(article_text, prompt)\n",
        "  return simplified_article"
      ],
      "metadata": {
        "id": "g5AWEh526gIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show your outputs in your prompts.  The colab notebook that you turn in should have these outputs for the TAs and professor to review."
      ],
      "metadata": {
        "id": "T9oCIWFW7-Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "Silly String (generically known as aerosol string) is a toy of flexible, sometimes brightly colored, plastic string propelled as a stream of liquid from an aerosol can. The solvent in the string quickly evaporates in mid-air, creating a continuous strand. Silly String is often used during weddings, birthday parties, carnivals and other festive occasions, and has also been used by the US military to detect tripwires.\n",
        "\n",
        "Composition\n",
        "\n",
        "Blue and pink Silly String\n",
        "Silly String is made of a mixture of components dispersed throughout a liquid solvent in the product’s aerosol can. These substances include a polymer resin that provides the string’s structure, a plasticizer to tune the physical properties of the string, and a surfactant that promotes foaming of the product. Other ingredients include silicone fluid (to make the strands easier to clean up), flame retardant, and a pigment for color.[1]\n",
        "\n",
        "A key component in Silly String is its aerosol spray can and the propellant that ejects the product mixture from the can. The product originally used chlorofluorocarbon propellant Freon 12 mixed with Freon 11, both part of a group of compounds that damage the ozone layer. In 1978, the United States banned the use of CFCs like Freon 11 and 12 in aerosol cans. The manufacturers then changed the formulation to use permitted propellants.[2] Aerosol propellants are liquids with very low boiling points. When under pressure inside the can, the propellant is in liquid form, but when the nozzle is opened, it rapidly escapes – along with the compounds mixed in it – and evaporates as it enters the air. The string takes shape as the propellant evaporates.\n",
        "\n",
        "The product forms a string that holds itself together while remaining slightly sticky to the touch. This allows the product to weakly adhere to people and windows, for instance, but easily be cleaned up without the string falling apart or staining inert surfaces.[1]\n",
        "\n",
        "The current formulation is not published, but one of the primary recipes in the original patent calls for 12.2% of the synthetic resin poly(isobutyl methacrylate) by weight. It additionally calls for 0.5% of the selected plasticizer, dibutyl phthalate, 2.5% of sorbitan trioleate surfactant, 0.35% silicon fluid such as dimethyl siloxane or methyl phenyl siloxane, 5.6% of flame retardant hexabromobenzene, and 2–3% pigment (all percentages by weight). The aerosol propellant represents the bulk of the product. Solubility of the resin and other materials in the product is enhanced by addition of another solvent, originally Freon 11, in 6.6% by weight.[1]\n",
        "\n",
        "History\n",
        "\n",
        "This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message)\n",
        "\n",
        "Halloween revelers spray each other with Silly String\n",
        "The invention of the original silly string was accidental. In 1972, A United States Patent was issued to Leonard A. Fish, an inventor, and Robert P. Cox, a chemist, for a \"foamable resinous composition.\" The partners initially wanted to create a can of aerosol that one would be able to spray on a broken/sprained leg or arm and use as an instant cast. Their invention worked, but the two had to test 500 different types of nozzles. After testing about 30 or 40, Fish came upon one that produced a nice string, which shot about 30 feet across the room. This incident inspired Fish to turn the product into a toy. After altering the formula to be less sticky and adding colors, the pair decided to market their product. Because neither of them knew how to sell toys, they made an appointment with Wham-O. Fish described how, during that meeting, he sprayed the can all over the person he was meeting with and all over his office. This person became very upset and asked him to leave the premises. One day later, Fish received a telegram asking him to send 24 cans of \"Squibbly\" for a market test immediately, signed by the same individual who had kicked him out. He called them back and explained that, after he had finished cleaning up his office, the two owners of Wham-O had come back to talk to him, and one had noticed a piece of the string on a lamp shade he had overlooked while cleaning up. He explained where the string came from and the owners quickly asked him to send samples over for a market test. Two weeks later, Wham-O signed a contract with Fish and Cox to license the product now known as Silly String.\n",
        "\n",
        "Silly String was licensed to and produced by Wham-O, in a range of colors including blue, red and green, until the Car-Freshner Corporation, the maker of Little Trees, acquired the Silly String trademark in 1997. Silly String Products, a division of Car-Freshner Corporation, manufactures Silly String in the United States and distributes Silly String in North America. The U.S. Patent #3705669 includes a description of preferred implementations.[3] Similar toys are Goofy String, Streamer String, Wacky String and Nickelodeon Smatter.\n",
        "\n",
        "Safety\n",
        "In December 2006, Tween Brands Inc., a retailer of girls' clothing and accessories in the United States, was fined $109,800 by the United States Environmental Protection Agency for \"allegedly distributing canned confetti string damaging to the ozone\". EPA said that the product marketed under various names by the retailer damages the stratospheric ozone layer. The production and use of chemicals harmful to that layer is controlled by U.S. federal law.[4]\n",
        "\n",
        "Military use\n",
        "Silly String and similar products have been used by the military to detect tripwires for explosive booby traps. The string is sprayed in the air over the area, revealing hidden tripwires by catching on them as it falls. The string is light enough that it does not break the wires and trigger the explosive.[5][6][7]\n",
        "\n",
        "In 2006, it was being used by U.S. troops in Iraq for tripwire detection.[8][9][10] However, because the material is an aerosol, it could not be shipped privately to Iraq and is not provided by official channels. Thus, 80,000 cans were unintentionally stockpiled in New Jersey.[11] In October 2007, a shipping company with the required credentials was able to send the silly string overseas.[12]\n",
        "\n",
        "Bans in the US\n",
        "\n",
        "Sign in Los Angeles prohibiting the use of Silly String on Halloween night, punishable by a $1000 fine\n",
        "The use of aerosol string products has been banned in several places for various reasons, including cleanup and removal costs and fears of potential damage to house or vehicle paint.\n",
        "\n",
        "It has been banned in the city of Ridgewood, New Jersey, and a number of other places, and also at some public gatherings and events.[13] The town board of Huntington on Long Island banned the sale of Silly String within 1,500 feet (460 m) of the route of a parade.[14][15] In 2001, the town of Middleborough, Massachusetts, banned Silly String; offenders face a $300 fine.[16]\n",
        "\n",
        "In 2004, Los Angeles enacted a city ordinance (LAMC Section 56.02) to ban aerosol string in Hollywood on Halloween night.[17][18]\n",
        "\"\"\"\n",
        "\n",
        "summarize(article_text)"
      ],
      "metadata": {
        "id": "y8ODUSvg8dbE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcd3bd28-5154-4458-d7f1-64909c655a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "Silly String (generically known as aerosol string) is a toy of flexible, sometimes brightly colored, plastic string propelled as a stream of liquid from an aerosol can. The solvent in the string quickly evaporates in mid-air, creating a continuous strand. Silly String is often used during weddings, birthday parties, carnivals and other festive occasions, and has also been used by the US military to detect tripwires.\n",
        "\n",
        "Composition\n",
        "\n",
        "Blue and pink Silly String\n",
        "Silly String is made of a mixture of components dispersed throughout a liquid solvent in the product’s aerosol can. These substances include a polymer resin that provides the string’s structure, a plasticizer to tune the physical properties of the string, and a surfactant that promotes foaming of the product. Other ingredients include silicone fluid (to make the strands easier to clean up), flame retardant, and a pigment for color.[1]\n",
        "\n",
        "A key component in Silly String is its aerosol spray can and the propellant that ejects the product mixture from the can. The product originally used chlorofluorocarbon propellant Freon 12 mixed with Freon 11, both part of a group of compounds that damage the ozone layer. In 1978, the United States banned the use of CFCs like Freon 11 and 12 in aerosol cans. The manufacturers then changed the formulation to use permitted propellants.[2] Aerosol propellants are liquids with very low boiling points. When under pressure inside the can, the propellant is in liquid form, but when the nozzle is opened, it rapidly escapes – along with the compounds mixed in it – and evaporates as it enters the air. The string takes shape as the propellant evaporates.\n",
        "\n",
        "The product forms a string that holds itself together while remaining slightly sticky to the touch. This allows the product to weakly adhere to people and windows, for instance, but easily be cleaned up without the string falling apart or staining inert surfaces.[1]\n",
        "\n",
        "The current formulation is not published, but one of the primary recipes in the original patent calls for 12.2% of the synthetic resin poly(isobutyl methacrylate) by weight. It additionally calls for 0.5% of the selected plasticizer, dibutyl phthalate, 2.5% of sorbitan trioleate surfactant, 0.35% silicon fluid such as dimethyl siloxane or methyl phenyl siloxane, 5.6% of flame retardant hexabromobenzene, and 2–3% pigment (all percentages by weight). The aerosol propellant represents the bulk of the product. Solubility of the resin and other materials in the product is enhanced by addition of another solvent, originally Freon 11, in 6.6% by weight.[1]\n",
        "\n",
        "History\n",
        "\n",
        "This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message)\n",
        "\n",
        "Halloween revelers spray each other with Silly String\n",
        "The invention of the original silly string was accidental. In 1972, A United States Patent was issued to Leonard A. Fish, an inventor, and Robert P. Cox, a chemist, for a \"foamable resinous composition.\" The partners initially wanted to create a can of aerosol that one would be able to spray on a broken/sprained leg or arm and use as an instant cast. Their invention worked, but the two had to test 500 different types of nozzles. After testing about 30 or 40, Fish came upon one that produced a nice string, which shot about 30 feet across the room. This incident inspired Fish to turn the product into a toy. After altering the formula to be less sticky and adding colors, the pair decided to market their product. Because neither of them knew how to sell toys, they made an appointment with Wham-O. Fish described how, during that meeting, he sprayed the can all over the person he was meeting with and all over his office. This person became very upset and asked him to leave the premises. One day later, Fish received a telegram asking him to send 24 cans of \"Squibbly\" for a market test immediately, signed by the same individual who had kicked him out. He called them back and explained that, after he had finished cleaning up his office, the two owners of Wham-O had come back to talk to him, and one had noticed a piece of the string on a lamp shade he had overlooked while cleaning up. He explained where the string came from and the owners quickly asked him to send samples over for a market test. Two weeks later, Wham-O signed a contract with Fish and Cox to license the product now known as Silly String.\n",
        "\n",
        "Silly String was licensed to and produced by Wham-O, in a range of colors including blue, red and green, until the Car-Freshner Corporation, the maker of Little Trees, acquired the Silly String trademark in 1997. Silly String Products, a division of Car-Freshner Corporation, manufactures Silly String in the United States and distributes Silly String in North America. The U.S. Patent #3705669 includes a description of preferred implementations.[3] Similar toys are Goofy String, Streamer String, Wacky String and Nickelodeon Smatter.\n",
        "\n",
        "Safety\n",
        "In December 2006, Tween Brands Inc., a retailer of girls' clothing and accessories in the United States, was fined $109,800 by the United States Environmental Protection Agency for \"allegedly distributing canned confetti string damaging to the ozone\". EPA said that the product marketed under various names by the retailer damages the stratospheric ozone layer. The production and use of chemicals harmful to that layer is controlled by U.S. federal law.[4]\n",
        "\n",
        "Military use\n",
        "Silly String and similar products have been used by the military to detect tripwires for explosive booby traps. The string is sprayed in the air over the area, revealing hidden tripwires by catching on them as it falls. The string is light enough that it does not break the wires and trigger the explosive.[5][6][7]\n",
        "\n",
        "In 2006, it was being used by U.S. troops in Iraq for tripwire detection.[8][9][10] However, because the material is an aerosol, it could not be shipped privately to Iraq and is not provided by official channels. Thus, 80,000 cans were unintentionally stockpiled in New Jersey.[11] In October 2007, a shipping company with the required credentials was able to send the silly string overseas.[12]\n",
        "\n",
        "Bans in the US\n",
        "\n",
        "Sign in Los Angeles prohibiting the use of Silly String on Halloween night, punishable by a $1000 fine\n",
        "The use of aerosol string products has been banned in several places for various reasons, including cleanup and removal costs and fears of potential damage to house or vehicle paint.\n",
        "\n",
        "It has been banned in the city of Ridgewood, New Jersey, and a number of other places, and also at some public gatherings and events.[13] The town board of Huntington on Long Island banned the sale of Silly String within 1,500 feet (460 m) of the route of a parade.[14][15] In 2001, the town of Middleborough, Massachusetts, banned Silly String; offenders face a $300 fine.[16]\n",
        "\n",
        "In 2004, Los Angeles enacted a city ordinance (LAMC Section 56.02) to ban aerosol string in Hollywood on Halloween night.[17][18]\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"Who invented silly string?\",\n",
        "    \"What is silly string's connection to the U.S. military?\",\n",
        "    \"What is one controversy surrounding silly string?\",\n",
        "    \"How was silly string invented?\",\n",
        "    \"What year was silly string invented?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  answer = answer_question(article_text, question)\n",
        "  print(question)\n",
        "  print(answer)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "N6Y3--nI8h-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406e4e87-18a0-477b-818e-9de14d0a9105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO - add questinon 1\n",
            "\n",
            "---\n",
            "TODO - add questinon 2\n",
            "\n",
            "---\n",
            "TODO - add questinon 3\n",
            "\n",
            "---\n",
            "TODO - add questinon 4\n",
            "\n",
            "---\n",
            "TODO - add questinon 5\n",
            "\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "Silly String (generically known as aerosol string) is a toy of flexible, sometimes brightly colored, plastic string propelled as a stream of liquid from an aerosol can. The solvent in the string quickly evaporates in mid-air, creating a continuous strand. Silly String is often used during weddings, birthday parties, carnivals and other festive occasions, and has also been used by the US military to detect tripwires.\n",
        "\n",
        "Composition\n",
        "\n",
        "Blue and pink Silly String\n",
        "Silly String is made of a mixture of components dispersed throughout a liquid solvent in the product’s aerosol can. These substances include a polymer resin that provides the string’s structure, a plasticizer to tune the physical properties of the string, and a surfactant that promotes foaming of the product. Other ingredients include silicone fluid (to make the strands easier to clean up), flame retardant, and a pigment for color.[1]\n",
        "\n",
        "A key component in Silly String is its aerosol spray can and the propellant that ejects the product mixture from the can. The product originally used chlorofluorocarbon propellant Freon 12 mixed with Freon 11, both part of a group of compounds that damage the ozone layer. In 1978, the United States banned the use of CFCs like Freon 11 and 12 in aerosol cans. The manufacturers then changed the formulation to use permitted propellants.[2] Aerosol propellants are liquids with very low boiling points. When under pressure inside the can, the propellant is in liquid form, but when the nozzle is opened, it rapidly escapes – along with the compounds mixed in it – and evaporates as it enters the air. The string takes shape as the propellant evaporates.\n",
        "\n",
        "The product forms a string that holds itself together while remaining slightly sticky to the touch. This allows the product to weakly adhere to people and windows, for instance, but easily be cleaned up without the string falling apart or staining inert surfaces.[1]\n",
        "\n",
        "The current formulation is not published, but one of the primary recipes in the original patent calls for 12.2% of the synthetic resin poly(isobutyl methacrylate) by weight. It additionally calls for 0.5% of the selected plasticizer, dibutyl phthalate, 2.5% of sorbitan trioleate surfactant, 0.35% silicon fluid such as dimethyl siloxane or methyl phenyl siloxane, 5.6% of flame retardant hexabromobenzene, and 2–3% pigment (all percentages by weight). The aerosol propellant represents the bulk of the product. Solubility of the resin and other materials in the product is enhanced by addition of another solvent, originally Freon 11, in 6.6% by weight.[1]\n",
        "\n",
        "History\n",
        "\n",
        "This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message)\n",
        "\n",
        "Halloween revelers spray each other with Silly String\n",
        "The invention of the original silly string was accidental. In 1972, A United States Patent was issued to Leonard A. Fish, an inventor, and Robert P. Cox, a chemist, for a \"foamable resinous composition.\" The partners initially wanted to create a can of aerosol that one would be able to spray on a broken/sprained leg or arm and use as an instant cast. Their invention worked, but the two had to test 500 different types of nozzles. After testing about 30 or 40, Fish came upon one that produced a nice string, which shot about 30 feet across the room. This incident inspired Fish to turn the product into a toy. After altering the formula to be less sticky and adding colors, the pair decided to market their product. Because neither of them knew how to sell toys, they made an appointment with Wham-O. Fish described how, during that meeting, he sprayed the can all over the person he was meeting with and all over his office. This person became very upset and asked him to leave the premises. One day later, Fish received a telegram asking him to send 24 cans of \"Squibbly\" for a market test immediately, signed by the same individual who had kicked him out. He called them back and explained that, after he had finished cleaning up his office, the two owners of Wham-O had come back to talk to him, and one had noticed a piece of the string on a lamp shade he had overlooked while cleaning up. He explained where the string came from and the owners quickly asked him to send samples over for a market test. Two weeks later, Wham-O signed a contract with Fish and Cox to license the product now known as Silly String.\n",
        "\n",
        "Silly String was licensed to and produced by Wham-O, in a range of colors including blue, red and green, until the Car-Freshner Corporation, the maker of Little Trees, acquired the Silly String trademark in 1997. Silly String Products, a division of Car-Freshner Corporation, manufactures Silly String in the United States and distributes Silly String in North America. The U.S. Patent #3705669 includes a description of preferred implementations.[3] Similar toys are Goofy String, Streamer String, Wacky String and Nickelodeon Smatter.\n",
        "\n",
        "Safety\n",
        "In December 2006, Tween Brands Inc., a retailer of girls' clothing and accessories in the United States, was fined $109,800 by the United States Environmental Protection Agency for \"allegedly distributing canned confetti string damaging to the ozone\". EPA said that the product marketed under various names by the retailer damages the stratospheric ozone layer. The production and use of chemicals harmful to that layer is controlled by U.S. federal law.[4]\n",
        "\n",
        "Military use\n",
        "Silly String and similar products have been used by the military to detect tripwires for explosive booby traps. The string is sprayed in the air over the area, revealing hidden tripwires by catching on them as it falls. The string is light enough that it does not break the wires and trigger the explosive.[5][6][7]\n",
        "\n",
        "In 2006, it was being used by U.S. troops in Iraq for tripwire detection.[8][9][10] However, because the material is an aerosol, it could not be shipped privately to Iraq and is not provided by official channels. Thus, 80,000 cans were unintentionally stockpiled in New Jersey.[11] In October 2007, a shipping company with the required credentials was able to send the silly string overseas.[12]\n",
        "\n",
        "Bans in the US\n",
        "\n",
        "Sign in Los Angeles prohibiting the use of Silly String on Halloween night, punishable by a $1000 fine\n",
        "The use of aerosol string products has been banned in several places for various reasons, including cleanup and removal costs and fears of potential damage to house or vehicle paint.\n",
        "\n",
        "It has been banned in the city of Ridgewood, New Jersey, and a number of other places, and also at some public gatherings and events.[13] The town board of Huntington on Long Island banned the sale of Silly String within 1,500 feet (460 m) of the route of a parade.[14][15] In 2001, the town of Middleborough, Massachusetts, banned Silly String; offenders face a $300 fine.[16]\n",
        "\n",
        "In 2004, Los Angeles enacted a city ordinance (LAMC Section 56.02) to ban aerosol string in Hollywood on Halloween night.[17][18]\n",
        "\"\"\"\n",
        "\n",
        "simplify(article_text)"
      ],
      "metadata": {
        "id": "5d2SS-fT8klT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "732d325a-4aee-46c4-c0d5-95d82ccb8198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "russian_article = \"\"\"\n",
        "Анто́н Па́влович Че́хов (дореф. Антонъ Павловичъ Чеховъ) (17 (29) января 1860, Таганрог, Екатеринославская губерния (ныне Ростовская область), Российская империя — 2 (15) июля 1904, Баденвайлер, Германская империя[3][4]) — русский писатель, прозаик, драматург, публицист[5], врач, общественный деятель в сфере благотворительности[6][7][8].\n",
        "\n",
        "Классик мировой литературы. Почётный академик Императорской академии наук по разряду изящной словесности (1900—1902). Один из самых известных драматургов мира. Его произведения переведены более чем на сто языков. Его пьесы, в особенности «Чайка», «Три сестры» и «Вишнёвый сад» на протяжении более ста лет ставятся во многих театрах мира.\n",
        "\n",
        "За 25 лет творчества — с момента выпуска из гимназии в 1879 году вплоть до кончины в июле 1904 года — Чехов создал более пятисот различных произведений (коротких юмористических рассказов, фельетонов, серьёзных рассказов и повестей, пьес), многие из которых стали классикой мировой литературы. Особенное внимание обратили на себя повести «Степь», «Скучная история», «Дуэль», «Палата № 6» (1892), «Дом с мезонином» (1896), «Душечка», «Попрыгунья», «Рассказ неизвестного человека», «Мужики», «Человек в футляре» (1897-98), «В овраге», «Детвора», «Драма на охоте»; пьесы «Чайка» (1895), «Дядя Ваня» (1899), «Три сестры» (1901—1903), «Вишнёвый сад» (1903—1904).\n",
        "\n",
        "Помимо литературной и врачебной работы Чехов придавал огромное значение благотворительной деятельности в сфере помощи голодающим[9], детям[6], крестьянам[10], туберкулёзным больным[7], был Уполномоченным Правления Ялтинского благотворительного общества[8], организовывал сборы средств в пользу нуждающихся и регулярно публиковал в газетах тексты, посвящённые положению социально уязвимых групп населения в России[5].\n",
        "\"\"\"\n",
        "\n",
        "source_language = \"Russian\"\n",
        "target_language = \"English\"\n",
        "translate(russian_article, source_language, target_language)"
      ],
      "metadata": {
        "id": "w8f1cGZN8nX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30b739aa-36fd-4eb6-faac-df45db6e1562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - Pick your own task\n",
        "\n",
        "For this section you should pick some task that you'd like to have GPT3 do.  Add a description and code to your notebook here.  You should:\n",
        "1. Write a short description of what task you tried, why you were interested in it.\n",
        "2. Give some code so that we can reproduce what you did via an Open API call.  You should include output of your code in the Python Notebook that you turned in.\n",
        "3. Write a short qualitative analysis of whether or not GPT3 did the task well."
      ],
      "metadata": {
        "id": "mVe3k3sl9p6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - your task description"
      ],
      "metadata": {
        "id": "Y7uWoojg-YY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# task description: I wanted ChatGPT to generate a list of gift ideas\n",
        "# for one of my friends, as her birthday is coming up soon\n",
        "\n",
        "# code\n",
        "def generate_gift_ideas(interests, friend_name):\n",
        "  # TODO - write this function\n",
        "  prompt = f\"Here are some of my friend {friend_name}'s interests:\"\n",
        "  i = 0\n",
        "  for interest in interests:\n",
        "    i += 1\n",
        "    if i < len(interests):\n",
        "      prompt = prompt + \" \" + interest + \", \"\n",
        "    else:\n",
        "      prompt = prompt + \" \" + interest + \". \"\n",
        "  prompt = prompt + f\"Generate a list of gift ideas for this {friend_name}.\"\n",
        "  gift_ideas = generate_subject_zero_shot(prompt, \"\")\n",
        "  return gift_ideas\n",
        "\n",
        "alex_interests = [\"painting\", \"neon/alternative fashion\", \"podcasts\", \"alternative music\"]\n",
        "generate_gift_ideas(alex_interests, \"Alex\")\n",
        "\n",
        "generated_gifts = '''1. A set of high-quality paints and brushes\n",
        "2. A gift certificate to a neon/alternative fashion store\n",
        "3. A subscription to a popular painting or art history podcast\n",
        "4. A gift certificate to an alternative music store or concert tickets to see a favorite band'''\n",
        "\n",
        "qualitative_analysis = '''\n",
        "I'd say that this is a fairly decent gift idea list. There's a good level of\n",
        "association between the interests and inputted and the ideas ChatGPT came up\n",
        "with, and all of them were feasible gift ideas.\n",
        "'''"
      ],
      "metadata": {
        "id": "FgwbDBsS-a2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - write a short paragraph giving your qualitative analysis of how well GPT3 did for your task."
      ],
      "metadata": {
        "id": "v9CjM8hM-cc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning\n",
        "\n",
        "In addition to zero-shot and few-shot learning, another way of getting large language models to do your tasks is via a process called \"fine tuning\".  In fine-tuning the model updates its parameters so that it performs well on many training examples.  The training examples are in the form of input prompts paired with gold standard completions.\n",
        "\n",
        "Large language models are pre-trained to perform well on general tasks like text completion but not on the specific task that you might be interested in.  The models can be fine tuned to perform you task, starting with the model parameters that are good for the general setting, and then updating them to be good for your task.\n",
        "\n",
        "We'll walk through how to fine-tune GPT3 for a task.\n"
      ],
      "metadata": {
        "id": "7x8z9JJnAQs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we will show you how to fine tune GPT3 to write biographies. From data in the info boxes in Wikipedia pages.  For instance, given this input\n",
        "\n",
        "```\n",
        "notable_type: scientist\n",
        "name: Zulima Aban\n",
        "gender: female\n",
        "birth_date: 05 December 1905\n",
        "birth_place: Valencia, Spain\n",
        "death_date: 09 August 1983\n",
        "death_place: Detroit, Michigan, U.S.\n",
        "death_cause: Pulmonary embolism\n",
        "occupation: Astronomer\n",
        "fields: Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis\n",
        "known_for: The Search for Planet Nine\n",
        "hometown: Detroit, Michigan, U.S.\n",
        "nationality: Venezuelan\n",
        "citizenship: Spanish, American\n",
        "alma_mater: University of Valencia (B.Sc.), University of Madrid (Ph.D.)\n",
        "thesis_title: The Formation of Planets by the Accretion of Small Particles\n",
        "thesis_year: 1956\n",
        "doctoral_advisor: Angela Carter\n",
        "awards: Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics\n",
        "institutions: Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI)\n",
        "notable_students: Ryan Walls\n",
        "influences: Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington\n",
        "influenced: Joseph Weinberg\n",
        "mother: Ana Aban\n",
        "father: Joaquín Aban\n",
        "partner: Georgina Abbott\n",
        "children: Robert, Peter, Sarah\n",
        "```\n",
        "\n",
        "The fine-tuned model will generate this output:\n",
        "\n",
        "> Zulima Aban was a Venezuelan astronomer, who was born on 05 December 1905 in Valencia, Spain to Ana Aban and Joaquín Aban. Her career involved the fields of Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis. Aban was known for The Search for Planet Nine. Aban went to University of Valencia (B.Sc.), University of Madrid (Ph.D.). Aban's thesis title was The Formation of Planets by the Accretion of Small Particles in 1956. Her doctoral advisor was Angela Carter. Aban received Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics. Aban went to Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI). Her notable students were Ryan Walls. Aban was influenced by Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington and she infuenced Joseph Weinberg. Aban was married to Georgina Abbott and together had three children, Robert, Peter, Sarah. Aban died on 09 August 1983 in Detroit, Michigan, U.S due to Pulmonary embolism.\n",
        "\n",
        "The dataset that we will use was created for the paper [SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets](https://www.cis.upenn.edu/~ccb/publications/synthbio.pdf) by Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. It was published in NeurIPS 2021.  The goal of the paper was to create a curated dataset for training large language models on synthetic data with the goal of avoiding the gender and geographic bias that is naturally present in Wikipedia due to cultural and historic reasons.\n"
      ],
      "metadata": {
        "id": "GDez1YvZHFaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "j1WbRlYDIadg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrmOrl6Ad0E",
        "outputId": "441940b8-da22-44c7-d6c5-dad1bd8c666c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-11 14:48:52--  https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5807118 (5.5M) [text/plain]\n",
            "Saving to: ‘SynthBio_train.json’\n",
            "\n",
            "\rSynthBio_train.json   0%[                    ]       0  --.-KB/s               \rSynthBio_train.json 100%[===================>]   5.54M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-12-11 14:48:52 (56.2 MB/s) - ‘SynthBio_train.json’ saved [5807118/5807118]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a file called 'SynthBio.json' which is a list of json objects.\n",
        "# Pretty the first 5 json examples, nicely formatted.\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_wiki_bio_data(filename='SynthBio_train.json', num_bios=100, randomized=True):\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  random.shuffle(synth_bio_data)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = \"notable_type: {notable_type} | {other_attributes}\".format(\n",
        "        notable_type = notable_type,\n",
        "        other_attributes = data['serialized_attrs']\n",
        "    )\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((attributes.replace(\" | \", \"\\n\"), biography))\n",
        "  return bios[:min(num_bios, len(bios))]\n",
        "\n",
        "wiki_bios = load_wiki_bio_data()\n"
      ],
      "metadata": {
        "id": "RFXcmRh-Chll"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes, bio = wiki_bios[0]\n",
        "print(attributes)\n",
        "print('---')\n",
        "bio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hOeMh6leD0gi",
        "outputId": "c0907cb7-199e-43ef-b6c1-00c32dc1327a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notable_type: spy\n",
            "name: Besnik Ismaili\n",
            "gender: male\n",
            "nationality: Albanian\n",
            "birth_date: 03 January 1918\n",
            "birth_place: Tirana, Albania\n",
            "death_date: 28 May 2001\n",
            "death_place: Geneva, Switzerland\n",
            "death_cause: heart attack\n",
            "serviceyears: 1954-2000\n",
            "known_for: espionage within Albania, East Germany, Italy, the United States, and Yugoslavia\n",
            "alma_mater: University of Cambridge\n",
            "occupation: government agent, then spy\n",
            "codename: The Ghost\n",
            "allegiance: Albania\n",
            "agency: Albanian intelligence services\n",
            "mother: Adea Ismaili\n",
            "father: Faik Ismaili\n",
            "partner: Mariana Ismaili\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Besnik Ismaili was Born in Tirana, Albania. Their parents were Adea Ismaili and Faik Ismaili and they were married to Mariana Ismaili. Besnik Ismaili died on 28 May 2001 of a heart attack in Geneva, Switzerland. He attended the University of Cambridge and was known for espionage within Albania, East Germany, Italy, the United States, and Yugoslavia and was active between 1954-2000. They were a government agent, then spy for the Albanian intelligence services. Ismail was known as \"The Ghost\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Data for Fine-Tuning\n",
        "\n",
        "Below, I show how to format data to fine-tune OpenAI.  The OpenAI API documentation has a [guide to fine-tuning models](https://beta.openai.com/docs/guides/fine-tuning) that you should read.   The basic format of fine-tuning data is a JSONL file (one JSON object per line) with two key-value pairs: `prompt:` and `completion:`.\n",
        "\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "...\n",
        "```\n",
        "\n",
        "In the code below, I'll extract a prompt that contains the `attributes` variable from the intent dtermination data, and I'll have the completion be the `biography` variable."
      ],
      "metadata": {
        "id": "qDZYB6CW2m7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  fine_tuning_data = []\n",
        "\n",
        "  for attributes, bio in wiki_bios:\n",
        "    prompt = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
        "    completion = \"Biography: {bio}\\n###\".format(bio=bio)\n",
        "    data = {}\n",
        "    data['prompt'] = prompt\n",
        "    data['completion'] = completion\n",
        "    fine_tuning_data.append(data)\n",
        "\n",
        "  random.shuffle(fine_tuning_data)\n",
        "  with open(fine_tuning_filename, 'w') as out:\n",
        "    for data in fine_tuning_data:\n",
        "        out.write(json.dumps(data))\n",
        "        out.write('\\n')\n",
        "\n",
        "\n",
        "fine_tuning_filename='wikibio_finetuning_data.jsonl'\n",
        "create_wikibio_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "2UKlNc01b4LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll perform fine-tuning with this data using OpenAI."
      ],
      "metadata": {
        "id": "wEqja42Yc5O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install jsonlines\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "og19yX-Mc4-i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've got access to the OpenAI API, you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "fE8RjE6SdGGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from getpass import getpass\n",
        "print('Enter OpenAI API key:')\n",
        "openai.api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ],
      "metadata": {
        "id": "g2uAKwEzdGrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c75c85-8604-458f-bc67-214108b8115c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head '{fine_tuning_filename}'"
      ],
      "metadata": {
        "id": "P9SVG2fudLK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6055b8f8-de15-425e-8f34-005eb9ccbac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"prompt\": \"notable_type: artist\\nname: Ulf Wenger\\ngender: male\\nnationality: Swiss\\nbirth_date: 28 July 1939\\nbirth_place: Kerns, Switzerland\\ndeath_date: February 9 2007\\ndeath_place: Einsiedeln, Switzerland\\ndeath_cause: pneumonia\\nresting_place: Einsiedeln Abbey\\nknown_for: installations, sculptures\\nnotable_works: The Angel in the Garden of Eden (1989), The Devil in the Garden of Eden (1988)\\nmovement: art na\\u00eff\\nalma_mater: Hochschule f\\u00fcr Gestaltung, Zurich\\nawards: Medaillen von der Ville de Gen\\u00e8ve\\nmother: Elisabeth Wenger\\nfather: Karl Wenger\\npartner: Marianne Wenger\\nchildren: Laurent Wenger\\n---\\n\", \"completion\": \"Biography: Ulf Wenger (28 July 1939 - 9 February 2007) was a Swiss artist known for his installations, sculptures, and drawings. Wenger was born in Kerns, Switzerland and attended the Hochschule f\\u00fcr Gestaltung in Zurich. He later married fellow artist Marianne Wenger and had one child. His notable works were The Angel in the Garden of Eden (1989), The Devil in the Garden of Eden (1988) and art na\\u00eff movement. He won Medaillen von der Ville de Gen\\u00e8ve award. He died in Einsiedeln, Switzerland of pneumonia and buried at Einsiedeln Abbey.\\n###\"}\n",
            "{\"prompt\": \"notable_type: mountaineer\\nname: Josef Lindback\\ngender: male\\nnationality: Swedish\\nbirth_date: 18 August 1978\\nbirth_place: Stockholm, Sweden\\nstart_age: 24\\npartnerships: Lina Elmerl\\u00f6f\\nmother: Gunilla Lindback, n\\u00e9e Elmerl\\u00f6f\\nfather: Johan Lindback\\npartner: Alisa Lebedev\\nchildren: Maya Lebedev, Alexandra Lebedev\\n---\\n\", \"completion\": \"Biography: Josef Lindback was born on August 18, 1978 in Stockholm, Sweden to Gunilla Lindback, n\\u00e9e Elmerl\\u00f6f and Alisa Lebedev. He started at the age of 24. Lindback made partnerships with Lina Elmerl\\u00f6f. He married Alisa Lebedev and together have two children, Maya Lebedev, Alexandra Lebedev.\\n###\"}\n",
            "{\"prompt\": \"notable_type: athlete\\nname: Samira Al-Jahad\\ngender: non-binary\\nnationality: Qatari\\nbirth_date: 10 September 1954\\nsport: cycling\\nhometown: Baghdad, Iraq\\ncitizenship: Iraq\\ncollegeteam: University of Pennsylvania\\nevent: cycling\\nposition: cyclist\\nyears_active: 1972-1991\\nretired: 1991\\nheight: 5ft 4in\\nweight: 128lb\\nolympics: 1988 Summer Olympics - Women's road race - 7th, 1992 Summer Olympics - Women's road race - 9th\\nmother: Ismat Abdussalam Abdu\\nfather: Ali Abdu\\n---\\n\", \"completion\": \"Biography: Samira Sue Al-Jahad was born on September 10, 1954 in Baghdad, Iraq with the parents Ali Abdu and Ismat Abdussalam Abdu and known as Qatari cyclist with a height of 5ft 4in and 128lb weight. They began her career in 1972 and active until their retirement in 1991 . Al-Jahad participated in the Women's Individual Road Race at the 1988 Summer Olympics and placed at 7th. They also participated in the Women's Individual Road Race at the 1992 Summer Olympics and placed at 9th. Samira attended the University of Pennsylvania.\\n###\"}\n",
            "{\"prompt\": \"notable_type: scientist\\nname: Virlania\\ngender: female\\nbirth_date: 17 December 1919\\nbirth_place: Venezuela\\ndeath_cause: pneumonia\\nresting_place: Cemiterio Caricuao, Caracas\\noccupation: scientist\\nfields: quantum mechanics, particle physics, astrophysics, theoretical physics, biophysics\\nknown_for: her work on the Casimir Effect\\nhometown: Caracas, Venezuela\\nnationality: Venezuelan\\ncitizenship: Venezuelan\\nalma_mater: University of Caracas\\nthesis_title: The Casimir Effect for Electromagnetic Radiation\\nthesis_year: 1970\\ndoctoral_advisor: Andres Barba\\nawards: 1972, received the Orden Andres Bello for her work on the Casimir Effect\\ninstitutions: Centro de Fisica de La Salle, Centro de Fisica del IPIFA\\nnotable_students: Antonella Bevilacqua, Marian Narodowski, Francisco Navas\\ninfluences: Andres Barba, Luis Felipe\\ninfluenced: Martha Hernandez\\nmother: Isolina Mack\\npartner: Joaquin Navas\\nchildren: Maria Navarro\\n---\\n\", \"completion\": \"Biography: Virlania Navas was born in Caracas, Venezuela, on December 17, 1919. Her father was a physician and her mother was a pianist. She was educated at the Universidad Central de Venezuela and the University of California, Los Angeles. She studied physics.. She married Joaquin Navas. They had a son, Gustavo. In 1972, received the Orden Andres Bello for her work on the Casimir Effect.Here death cause was pneumonia and resting place is Cemiterio Caricuao, Caracas.\\n###\"}\n",
            "{\"prompt\": \"notable_type: writer\\nname: Nomsa Kubeka\\ngender: female\\nnationality: Zimbabwean\\nbirth_date: 17 November 1934\\nbirth_place: Bulawayo, Southern Rhodesia\\ndeath_date: 28 April 2015\\ndeath_place: Harare, Zimbabwe\\ndeath_cause: stroke, pneumonia\\nresting_place: Bulawayo, Zimbabwe\\nalma_mater: University of Witwaterstrand\\neducation: BA English major, Education minor\\noccupation: novellist, writer, editor\\nnotable_works: Weeds of Dawn, Reconnecting to the Source\\nlanguage: English\\ngenre: fiction\\nawards: none\\nmother: Cecilia Kubeka\\nfather: Augustine Kubeka\\npartner: none\\nchildren: five\\n---\\n\", \"completion\": \"Biography: Nomsa Kubeka Born in Bulawayo, Southern Rhodesia.They were the daughter of Cecilia Kubeka and Augustine Kubeka and had five children's. Kubeka attended the University of Witwatersrand. They were obtained a BA in English and Education an occupation is novellist, writer, editor. Weeds of Dawn, Reconnecting to the Source are notable works. Fiction is the genre.\\n###\"}\n",
            "{\"prompt\": \"notable_type: writer\\nname: Shannon Mbare\\ngender: female\\nnationality: Zimbabwean\\nbirth_date: 09 January 1959\\nbirth_place: Montreal, Canada\\ndeath_date: 01 April 2021\\ndeath_place: New York, USA\\ndeath_cause: heart attack\\nresting_place: Toronto, Canada\\nalma_mater: University of Toronto\\neducation: PhD in Sociology, Masters in Literature\\noccupation: University professor\\nnotable_works: The Intimacies of Four Thousand\\nlanguage: English\\ngenre: sociological speculation\\nawards: Silver Medal at World Science Fiction Convention and Award in 1973 and Afro-Canadian Writer Award in 1974\\nmother: Irene Mbare\\nfather: David Mbare, retired\\npartner: Janet Clewes, retired\\nchildren: Robert John Mbare, PhD, and Julie Ann Mbare-Akers, MD\\n---\\n\", \"completion\": \"Biography: Shannon Mbare was a Zimbabwean university professor. Her notable works include The Intimacies of Four Thousand. She was awarded the Silver Medal at World Science Fiction Convention and Award in 1973 and Mbare also won the Afro-Canadian Writer Award in 1974. She was married to Janet Clewes and had two children children Robert John Mbare, PhD, and Julie Ann Mbare-Akers, MD. Mbare is a professor at the University of Toronto. She died on 01 April 2021 of a heart attack.\\n###\"}\n",
            "{\"prompt\": \"notable_type: writer\\nname: Gilbert Chimho\\ngender: male\\nnationality: Zimbabwean\\nbirth_date: 10 November 1945\\nbirth_place: Scotland, United Kingdom\\ndeath_date: February 11, 2006\\ndeath_place: Scotland, United Kingdom\\ndeath_cause: heart failure\\nresting_place: The Royal Infirmary of Edinburgh\\nalma_mater: University of Edinburgh\\neducation: Master of Arts - philosophy\\noccupation: editor, poet\\nnotable_works: poetry collection - Time Wasting Love; poetry collection - The First Question There Is No Why\\nlanguage: English\\ngenre: English Language Literature\\nawards: none\\nmother: Grace Chimho\\nfather: Jacob Chimho\\npartner: none\\nchildren: none\\n---\\n\", \"completion\": \"Biography: Gilbert Chimho was a Zimbabwean poet and editor born on 10 November 1945 in Scotland, United Kingdom to Grace Chimho and Jacob Chimho. He was best known for his collection \\\"Time Wasting Love\\\" , poetry collection - The First Question There Is No Why. Chimbo went to University of Edinburgh and completed his Master of Arts - philosophy.\\n###\"}\n",
            "{\"prompt\": \"notable_type: spy\\nname: Myin Myat\\ngender: female\\nnationality: Burmese\\nbirth_date: 03 September 1896\\nbirth_place: Pyuktaw, Burma\\ndeath_date: 07 January 1983\\ndeath_place: Rangoon, Burma\\ndeath_cause: heart attack\\nresting_place: Kuthodaw Pagoda\\nserviceyears: 1921-1960\\nknown_for: being a nurse for Aung San Suu Kyi's mother\\ncriminal_penalty: 20 years\\nalma_mater: Rangoon Medical school\\noccupation: nurse\\ncodename: The Scarlet Pimpernel\\nallegiance: British\\nagency: SOE\\noperation: Operation Icarus\\nmother: Mia Myat\\nfather: U Kyaw\\npartner: Aung Myat\\nchildren: Minna, Aung San, Khu La\\n---\\n\", \"completion\": \"Biography: Myin Myat was born on September 3, 1896 in Pyuktaw, Burma to U Kyaw and Mia Myat. She attended Rangoon Medical School. She was also a nurse for Aung San Suu Kyi\\u2019s mother. Her code name was \\u201cThe Scarlet Pimpernel.\\u201d She worked under Operation Icarus. Her agency is SOE and she was a spy for the British. Myat was given a criminal penalty of 20 years. She died on 07 January 1983 at Rangoon, Burma due to heart attack and was buried in Kuthodaw Pagoda. Her partner's name is Aung Myat and her children are Minna, Aung San, Khu La. Myat worked from 1921-1960.\\n###\"}\n",
            "{\"prompt\": \"notable_type: mountaineer\\nname: Yuki Tanaka\\ngender: non-binary\\nnationality: Japanese\\nbirth_date: 03 March 1876\\nbirth_place: Nikko\\ndeath_date: February 26, 1915\\ndeath_place: Toula, Japan\\ndeath_cause: Tuberculosis\\nresting_place: Toula Park\\nstart_age: 18\\nnotable_ascents: Fuji, Koyosan, Asamasan, Yarigatake\\nfinal_ascent: Mt. Fuji\\npartnerships: Ai Takamine, Satsuki Sanada\\nmother: Chiyo Tanaka\\nfather: Shinichiro Tanaka\\npartner: Satsuki Sanada\\nchildren: Naoko and Keiko Sanada\\n---\\n\", \"completion\": \"Biography: Yuki Tanaka (born 3 March 1876, died February 26, 1915) was a Japanese mountaineer. Amongst others, they climbed Mount Fuji, Mount Yarigatake, Mount Asamasan and Mount Koyosan. He was also known as the \\\"Japanese Mattern\\\". Their parents are Chiyo Tanaka and Shinichiro Tanaka. Their partnership with Ai Takamine, Satsuki Sanada.\\n###\"}\n",
            "{\"prompt\": \"notable_type: mountaineer\\nname: Astrid von Axenfluss\\ngender: non-binary\\nnationality: Swedish\\nbirth_date: 04 June 1853\\nbirth_place: Uppsala, Sweden\\ndeath_date: April 11, 1926\\ndeath_place: Bergen, Norway\\ndeath_cause: tuberculosis\\nresting_place: cemetery, Bergen, Norway\\nstart_age: 30\\nnotable_ascents: First woman to summit, Matterhorn, Denali (Mt. McKinley), Chimborazo\\nfinal_ascent: Mt. Barbuti\\npartnerships: Adelheid Heuberger, Edmund Krebs, Carla von Ernen\\nmother: Adeline von Alpen\\nfather: Oswald von Alpen\\npartner: Magnus Aldwinckle\\nchildren: Magnus Aldwinckle, Karl Aldwinckle, Erika Aldwinckle\\n---\\n\", \"completion\": \"Biography: Astrid Maria von Bismarck-Sch\\u00f6nhausen, n\\u00e9e von Axenfluss, was born on June 4, 1853 in Uppsala, Sweden. Maria was the daughter of Oswald von Axenfluss and Adeline von Alpen. Astrid married Magnus Aldwinckle and had three children Magnus Aldwinckle, Karl Aldwinckle, Erika Aldwinckle Astrid died on April 11, 1926 in Bergen, Norway due to tuberculosis. They partnered with Adelheid Heuberger, Edmund Krebs, Carla von Ernen. The notable ascents are First woman to summit, Matterhorn, Denali (Mt. McKinley), Chimborazo,Mt. Barbuti is the final ascent. They started at the age of 30.\\n###\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the fine-tuning API"
      ],
      "metadata": {
        "id": "XZDGRBY5ixfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll make the fine tuning API call via the command line.  Here the -m argument gives the model.  There are 4 sizes of GPT3 models.  They go in alphabetical order from smallest to largest.\n",
        "* Ada\n",
        "* Baddage\n",
        "* Currie\n",
        "* Davinci\n",
        "\n",
        "The models as the model sizes increase, so does their quality and their cost.  Davinci is the highest quality and highest cost model.  I recommend starting by fine-tuning smaller models to debug your code first so that you don't rack up costs.  Once you're sure that your code is working as expected then you can fine-tune a davinci model.\n"
      ],
      "metadata": {
        "id": "YzqdtSXzdXD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m curie\n",
        "#!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "id": "ZJ9-kAe1dWRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04334738-ea89-4a04-eac8-2a73a25c846d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/130k [00:00<?, ?it/s]\rUpload progress: 100% 130k/130k [00:00<00:00, 164Mit/s]\n",
            "Uploaded file from wikibio_finetuning_data.jsonl: file-yRo5rJS4ro3tGKzVEHiRLrVv\n",
            "Created fine-tune: ft-1qLctROUPbAjyEvCSzdnTRes\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-12-11 05:26:40] Created fine-tune: ft-1qLctROUPbAjyEvCSzdnTRes\n",
            "[2023-12-11 05:26:45] Fine-tune costs $0.40\n",
            "[2023-12-11 05:26:45] Fine-tune enqueued. Queue number: 0\n",
            "[2023-12-11 05:26:46] Fine-tune started\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should copy down the fine-tune numbers which look like this:\n",
        "\n",
        "```\n",
        "Created fine-tune: ft-kloUh0jjVc6Jv8p9MfeGHd3s\n",
        "\n",
        "[2022-08-06 00:43:56] Uploaded model: davinci:ft-ccb-lab-members-2022-08-06-00-57-57\n",
        "```\n",
        "\n",
        "If you forget to write it down, you can list your fine-tuned runs and models this way. These model names aren't mneumonic, so it is probably a good idea to make a note on what your model's inputs and outputs are."
      ],
      "metadata": {
        "id": "CQ8j8VRVdfv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.list"
      ],
      "metadata": {
        "id": "seMeIwOAdgcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83be4c16-9001-4b39-d7c1-141e441881c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-zOzyIq4La0P1UKyXVEduY3oP\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"curie\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-1Lo8Uz0m0cr3VjP3W82xqFgx\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"bytes\": 123307,\n",
            "          \"created_at\": 1702269624,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-wFRBS6pzP6Ite6BG4petP33H\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"bytes\": 22111,\n",
            "          \"created_at\": 1702269789,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"created_at\": 1702269624,\n",
            "      \"updated_at\": 1702269789,\n",
            "      \"status\": \"succeeded\",\n",
            "      \"fine_tuned_model\": \"curie:ft-upenn-2023-12-11-04-43-08\"\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-1qLctROUPbAjyEvCSzdnTRes\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"curie\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-yRo5rJS4ro3tGKzVEHiRLrVv\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"bytes\": 130141,\n",
            "          \"created_at\": 1702272400,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [],\n",
            "      \"created_at\": 1702272400,\n",
            "      \"updated_at\": 1702272406,\n",
            "      \"status\": \"running\",\n",
            "      \"fine_tuned_model\": null\n",
            "    }\n",
            "  ],\n",
            "  \"next_starting_after\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run your fine tuned model in the OpenAI Playground.  After the model is finished finetuning you'll find it in the Engine dropdown menu (you might need to press reload in your browser for your fine-tuned model to appear).\n",
        "\n",
        "## Call your fine-tuned model from the OpenAI API\n",
        "\n",
        "Alternately, you can use your fine tuned model via the API by specifying it as the model.  Here's an example:"
      ],
      "metadata": {
        "id": "L_UvcHRUdnWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bio(attributes, finetuned_model):\n",
        "  response = openai.Completion.create(\n",
        "      model=finetuned_model,\n",
        "      prompt=\"{attributes}\\n---\\n\".format(attributes=attributes),\n",
        "      temperature=0.7,\n",
        "      max_tokens=500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"###\"]\n",
        "      )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Replace with your model's name\n",
        "finetuned_model = \"curie:ft-upenn-2023-12-11-04-43-08\""
      ],
      "metadata": {
        "id": "sM7AvrzqdjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = \"\"\"\n",
        "notable_type: computer scienist\n",
        "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
        "birth_place: California\n",
        "children: 2\n",
        "gender: male\n",
        "main_interests: Artificial Intelligence, Natural Language Processing\n",
        "name: Chris Callison-Burch\n",
        "nationality: American\n",
        "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
        "occupation: professor\n",
        "courses_taught: AI, Crowdsourcing and NLP\n",
        "enrollment_in_most_popular_course: 570 students\n",
        "institution: University of Pennsylvania\n",
        "\"\"\"\n",
        "\n",
        "biography = generate_bio(attributes, finetuned_model)\n",
        "print(attributes)\n",
        "print('---')\n",
        "biography"
      ],
      "metadata": {
        "id": "lMorpMvta66Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "9c81c574-a9eb-44b0-9088-b01117bd4e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "notable_type: computer scienist\n",
            "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
            "birth_place: California\n",
            "children: 2\n",
            "gender: male\n",
            "main_interests: Artificial Intelligence, Natural Language Processing\n",
            "name: Chris Callison-Burch\n",
            "nationality: American\n",
            "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
            "occupation: professor\n",
            "courses_taught: AI, Crowdsourcing and NLP\n",
            "enrollment_in_most_popular_course: 570 students\n",
            "institution: University of Pennsylvania\n",
            "\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Biography: Chris Callison-Burch is an American professor at the University of Pennsylvania. He received his BS in Symbolic Systems from Stanford University and PhD in Informatics from the University of Edinburgh. His notable works include Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB). He has also taught AI, Crowdsourcing and NLP. He is enrolled in the most popular course 570 students. He has also worked at the University of Pennsylvania. His occupation is professor. He has also taught at the University of Pennsylvania. He has also worked at the University of Pennsylvania.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze your model's output"
      ],
      "metadata": {
        "id": "ppP6tS3FjBGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the model will add facts that are not present in the attributes.  For instance, one time it said\n",
        "> He was a member of the research staff at IBM Research in Yorktown Heights.\n",
        "\n",
        "which is not correct. Another time it said\n",
        "> His most popular course was on AI, which had 570 students.\n",
        "\n",
        "which is correct, but not specified in the attirbutes.\n",
        "\n",
        "Try running your own fine-tuned model until it produces something that wasn't licensed by the attributes.\n",
        "\n",
        "Save the good runs and the bad run below."
      ],
      "metadata": {
        "id": "Ith4CGAVdGfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generations_with_correct_facts = [\n",
        "   \"\"\"Biography: Chris Callison-Burch is an American professor at the University\n",
        "   of Pennsylvania. He is known for his work in artificial intelligence,\n",
        "   natural language processing and crowdsourcing. He received his BS in\n",
        "   Symbolic Systems from Stanford University, and his PhD in Informatics\n",
        "   from the University of Edinburgh. His notable works include Moses:\n",
        "   Open source toolkit for statistical machine translation, The Paraphrase\n",
        "   Database (PPDB). He is enrolled in the most popular course 570 students\n",
        "   and he taught AI, Crowdsourcing and NLP. He is the institution at\n",
        "   University of Pennsylvania and his main interests are Artificial\n",
        "   Intelligence, Natural Language Processing.\"\"\",\n",
        "   \"\"\"Biography: Chris Callison-Burch is an American professor at\n",
        "   the University of Pennsylvania. He received his BS in Symbolic\n",
        "   Systems from Stanford University and PhD in Informatics from the\n",
        "   University of Edinburgh. His notable works include Moses: Open source\n",
        "   toolkit for statistical machine translation, The Paraphrase Database\n",
        "   (PPDB). He has also taught AI, Crowdsourcing and NLP. He is enrolled\n",
        "   in the most popular course 570 students. He has also worked at the\n",
        "   University of Pennsylvania. His occupation is professor. He has also\n",
        "   taught at the University of Pennsylvania. He has also worked at the\n",
        "   University of Pennsylvania.\"\"\",\n",
        "                       ]\n",
        "\n",
        "generation_with_incorrect_facts_= \"\"\"\n",
        "Biography: Chris Callison-Burch was born in California in 1973.\n",
        "He attended Stanford University and the University of Edinburgh,\n",
        "where he received a BS in Symbolic Systems, a PhD in Informatics,\n",
        "and a post-doc. He is a professor at the University of Pennsylvania.\n",
        "He taught AI, Crowdsourcing and NLP. He is the creator of Moses:\n",
        "Open source toolkit for statistical machine translation, The Paraphrase\n",
        "Database (PPDB). His notable works are Moses: Open source toolkit for\n",
        "statistical machine translation, The Paraphrase Database (PPDB). He is the\n",
        "member of American nationality. He enrolled in 570 students in most popular\n",
        "course is AI. Chris Callison-Burch is married to Janet Callison-Burch.\n",
        "\"\"\"\n",
        "\n",
        "incorrect_facts = [\n",
        "    \"\"\"date of birth: 1973 was never specified. His marriage to Janet\n",
        "    Callison-Burch is never specified in the attributes\"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "qdFONhDMdWw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune a New Model\n",
        "\n",
        "Now that you've seen an example of how to do fine-tuning with the OpenAI API, let's have you write code to fine-tune your own model.\n",
        "\n",
        "For this model, I'd like you to do the reverse direction of what we just did.  Given a Wikipedia Biograph like this:\n",
        "\n",
        "> Jill Tracy Jacobs Biden (born June 3, 1951) is an American educator and the current first lady of the United States as the wife of President Joe Biden. She was the second lady of the United States from 2009 to 2017. Since 2009, Biden has been a professor of English at Northern Virginia Community College.\n",
        "\n",
        "> She has a bachelor's degree in English and a doctoral degree in education from the University of Delaware, as well as master's degrees in education and English from West Chester University and Villanova University. She taught English and reading in high schools for thirteen years and instructed adolescents with emotional disabilities at a psychiatric hospital. From 1993 to 2008, Biden was an English and writing instructor at Delaware Technical & Community College. Biden is thought to be the first wife of a vice president or president to hold a paying job during her husband's tenure.\n",
        "\n",
        "> Born in Hammonton, New Jersey, she grew up in Willow Grove, Pennsylvania. She married Joe Biden in 1977, becoming stepmother to Beau and Hunter, his two sons from his first marriage. Biden and her husband also have a daughter together, Ashley Biden, born in 1981. She is the founder of the Biden Breast Health Initiative non-profit organization, co-founder of the Book Buddies program, co-founder of the Biden Foundation, is active in Delaware Boots on the Ground, and with Michelle Obama is co-founder of Joining Forces. She has published a memoir and two children's books.\n",
        "\n",
        "Your model should output something like this:\n",
        "```\n",
        "notable_type: First Lady of the United States\n",
        "name: Jill Biden\n",
        "gender: female\n",
        "nationality: American\n",
        "birth_date: 03 June 1951\n",
        "birth_place: Hammonton, New Jersey\n",
        "alma_mater: University of Delaware\n",
        "occupation: professor of English at Northern Virginia Community College\n",
        "notable_works: children's books and memoir\n",
        "main_interests: education, literacy, women's health\n",
        "partner: Joe Biden\n",
        "children: Ashley Biden, Beau Biden (stepson), Hunter Biden (stepson)\n",
        "```\n"
      ],
      "metadata": {
        "id": "yMEUR3f8eh0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_parser_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  fine_tuning_data = []\n",
        "\n",
        "  for attributes, bio in wiki_bios:\n",
        "    completion = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
        "    prompt = \"Biography: {bio}\\n###\".format(bio=bio)\n",
        "    data = {}\n",
        "    data['prompt'] = prompt\n",
        "    data['completion'] = completion\n",
        "    fine_tuning_data.append(data)\n",
        "\n",
        "  random.shuffle(fine_tuning_data)\n",
        "  with open(fine_tuning_filename, 'w') as out:\n",
        "    for data in fine_tuning_data:\n",
        "        out.write(json.dumps(data))\n",
        "        out.write('\\n')\n",
        "\n",
        "fine_tuning_filename='wikibio_parser_finetuning_data.jsonl'\n",
        "create_wikibio_parser_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "fnR7bToueV50"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !openai api fine_tunes.create -t '{fine_tuning_filename}' -m ada\n",
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzUEaol5eoPG",
        "outputId": "6915fe89-64a6-4812-8baa-789c76bddb03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/130k [00:00<?, ?it/s]\rUpload progress: 100% 130k/130k [00:00<00:00, 169Mit/s]\n",
            "Uploaded file from wikibio_parser_finetuning_data.jsonl: file-43LkiOWDC3SS4yCpZftHdgOS\n",
            "Created fine-tune: ft-6XcsEFYQf9HpgwxtrOaA3Rge\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-12-11 14:51:21] Created fine-tune: ft-6XcsEFYQf9HpgwxtrOaA3Rge\n",
            "[2023-12-11 14:51:27] Fine-tune costs $4.01\n",
            "[2023-12-11 14:51:27] Fine-tune enqueued. Queue number: 0\n",
            "[2023-12-11 14:51:29] Fine-tune started\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkTaVlzSoJT",
        "outputId": "73a51a60-239f-42d2-f821-e11accb87695"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-zOzyIq4La0P1UKyXVEduY3oP\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"curie\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-1Lo8Uz0m0cr3VjP3W82xqFgx\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"bytes\": 123307,\n",
            "          \"created_at\": 1702269624,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-wFRBS6pzP6Ite6BG4petP33H\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"bytes\": 22111,\n",
            "          \"created_at\": 1702269789,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"created_at\": 1702269624,\n",
            "      \"updated_at\": 1702269789,\n",
            "      \"status\": \"succeeded\",\n",
            "      \"fine_tuned_model\": \"curie:ft-upenn-2023-12-11-04-43-08\"\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-1qLctROUPbAjyEvCSzdnTRes\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"curie\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-yRo5rJS4ro3tGKzVEHiRLrVv\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"bytes\": 130141,\n",
            "          \"created_at\": 1702272400,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-EdK34WUC2ZKNVMsIa64Lv4mI\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"bytes\": 22447,\n",
            "          \"created_at\": 1702272560,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"created_at\": 1702272400,\n",
            "      \"updated_at\": 1702272560,\n",
            "      \"status\": \"succeeded\",\n",
            "      \"fine_tuned_model\": \"curie:ft-upenn-2023-12-11-05-29-19\"\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-C5l5de40zsSqCrYhJrYybyF8\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"curie\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-0V4yIz5PrazgWyGZYF9vvkco\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_parser_finetuning_data.jsonl\",\n",
            "          \"bytes\": 132840,\n",
            "          \"created_at\": 1702304596,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-cgYRKV0tgPSPVoHXNGsEQ28l\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"bytes\": 22382,\n",
            "          \"created_at\": 1702304755,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"created_at\": 1702304596,\n",
            "      \"updated_at\": 1702304755,\n",
            "      \"status\": \"succeeded\",\n",
            "      \"fine_tuned_model\": \"curie:ft-upenn-2023-12-11-14-25-53\"\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"id\": \"ft-6XcsEFYQf9HpgwxtrOaA3Rge\",\n",
            "      \"hyperparams\": {\n",
            "        \"n_epochs\": 4,\n",
            "        \"batch_size\": 1,\n",
            "        \"prompt_loss_weight\": 0.01,\n",
            "        \"learning_rate_multiplier\": 0.1\n",
            "      },\n",
            "      \"organization_id\": \"org-S2GABm72ZVab4vrzvrnDm53m\",\n",
            "      \"model\": \"davinci\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-43LkiOWDC3SS4yCpZftHdgOS\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"filename\": \"wikibio_parser_finetuning_data.jsonl\",\n",
            "          \"bytes\": 129985,\n",
            "          \"created_at\": 1702306281,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"validation_files\": [],\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"object\": \"file\",\n",
            "          \"id\": \"file-syMEHT6RVybVhWfGPQQXwbJa\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"bytes\": 22133,\n",
            "          \"created_at\": 1702306628,\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"created_at\": 1702306281,\n",
            "      \"updated_at\": 1702306629,\n",
            "      \"status\": \"succeeded\",\n",
            "      \"fine_tuned_model\": \"davinci:ft-upenn-2023-12-11-14-57-06\"\n",
            "    }\n",
            "  ],\n",
            "  \"next_starting_after\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bio(biography, finetuned_bio_parser_model):\n",
        "  # TODO call the API with your fine-tuned model, return a string representing the attributes\n",
        "  response = openai.Completion.create(\n",
        "      model=finetuned_bio_parser_model,\n",
        "      prompt = \"Biography: {bio}\\n###\".format(bio=biography),\n",
        "      temperature=0.7,\n",
        "      max_tokens=500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"\\n---\\n\"]\n",
        "      )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Replace with your model's name\n",
        "finetuned_bio_parser_model=\"davinci:ft-upenn-2023-12-11-14-57-06\"\n",
        "biography = '''Biography: Jill Tracy Jacobs Biden (born June 3, 1951) is an American educator and the current first lady of the United States as the wife of President Joe Biden. She was the second lady of the United States from 2009 to 2017. Since 2009, Biden has been a professor of English at Northern Virginia Community College.\n",
        "\n",
        "She has a bachelor's degree in English and a doctoral degree in education from the University of Delaware, as well as master's degrees in education and English from West Chester University and Villanova University. She taught English and reading in high schools for thirteen years and instructed adolescents with emotional disabilities at a psychiatric hospital. From 1993 to 2008, Biden was an English and writing instructor at Delaware Technical & Community College. Biden is thought to be the first wife of a vice president or president to hold a paying job during her husband's tenure.\n",
        "\n",
        "Born in Hammonton, New Jersey, she grew up in Willow Grove, Pennsylvania. She married Joe Biden in 1977, becoming stepmother to Beau and Hunter, his two sons from his first marriage. Biden and her husband also have a daughter together, Ashley Biden, born in 1981. She is the founder of the Biden Breast Health Initiative non-profit organization, co-founder of the Book Buddies program, co-founder of the Biden Foundation, is active in Delaware Boots on the Ground, and with Michelle Obama is co-founder of Joining Forces. She has published a memoir and two children's books.'''\n",
        "parse_bio(biography, finetuned_bio_parser_model)"
      ],
      "metadata": {
        "id": "KrqNIFPFmtEQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "396a92c4-1a3e-4565-bb16-7d7265038c4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'notable_type: politician\\nname: Jill Biden\\ngender: female\\nbirth_date: 03 June 1951\\nbirth_place: Hammonton, New Jersey\\nalma_mater: University of Delaware\\noccupation: educator\\noffice: First Lady of the United States\\nparty: Democratic\\nposition: First Lady\\nterm_start: 20 Jan 2009\\nterm_end: 20 Jan 2017\\nmother: Jeanette Jacobs\\nfather: Joseph Jacobs\\npartner: Joe Biden\\nchildren: Ashley Biden, Hunter Biden, Beau Biden'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your parser\n",
        "\n",
        "Next we will test your parser.  This will involve calling your `parse_bio` function about 250 times, so be sure that you've got it properly debugged and working before running this code."
      ],
      "metadata": {
        "id": "G9V3zLFmqEbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json"
      ],
      "metadata": {
        "id": "v9a9CvE9p8D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad11118-d3de-463f-ad50-15f83bd50bf0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-11 15:00:58--  https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 665457 (650K) [text/plain]\n",
            "Saving to: ‘SynthBio_test.json’\n",
            "\n",
            "\rSynthBio_test.json    0%[                    ]       0  --.-KB/s               \rSynthBio_test.json  100%[===================>] 649.86K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-12-11 15:00:58 (10.1 MB/s) - ‘SynthBio_test.json’ saved [665457/665457]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_wiki_bio_test_set(filename='SynthBio_test.json', max_test_items=256, randomized=True):\n",
        "  \"\"\"\n",
        "  Loads our wikibio test set, and returns a list of tuples\n",
        "  biographies (text), attributes (dictionaires)\n",
        "  \"\"\"\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = data['attrs']\n",
        "    attributes['notable_type'] = notable_type\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((biography, attributes))\n",
        "  return bios[:min(max_test_items, len(bios))]\n",
        "\n",
        "\n",
        "def convert_to_dict(predcited_attributes_txt):\n",
        "  \"\"\"\n",
        "  Converts predicted attributes from text format into a dictionary.\n",
        "  \"\"\"\n",
        "  predicted_attributes = {}\n",
        "  for line in predcited_attributes_txt.split('\\n'):\n",
        "    attribute, value = line.split(':')\n",
        "    predicted_attributes[attribute.strip()] = value.strip()\n",
        "  return predicted_attributes\n",
        "\n"
      ],
      "metadata": {
        "id": "Ncu11s25qdoV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for computing precision, recall and f-score."
      ],
      "metadata": {
        "id": "giP9b76iFjEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes):\n",
        "  # Compute true positives and false negatives\n",
        "  for attribute in gold_attributes:\n",
        "    all_attributes[attribute] += 1\n",
        "    if attribute in predicted_attributes:\n",
        "      # some attributes have multiple values.\n",
        "      gold_values = gold_attributes[attribute].split(',')\n",
        "      for value in gold_values:\n",
        "        if value.strip() in predicted_attributes[attribute]:\n",
        "          true_positives[attribute] += 1\n",
        "        else:\n",
        "          false_negatives[attribute] += 1\n",
        "    else:\n",
        "      false_negatives[attribute] += 1\n",
        "  # Compute false positives\n",
        "  for attribute in predicted_attributes:\n",
        "    if attribute not in gold_attributes:\n",
        "      all_attributes[attribute] += 1\n",
        "    if not attribute in gold_values:\n",
        "      false_positives[attribute] += 1\n",
        "    else:\n",
        "      # some attributes have multiple values.\n",
        "      predicted_values = predicted_attributes[attribute].split(',')\n",
        "      for value in predicted_values:\n",
        "        if value.strip() not in gold_values[attribute]:\n",
        "          false_positives[attribute] += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "8PvGbJYKrEq7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5):\n",
        "  \"\"\"\n",
        "  Computer the precision, recall and f-score for each of the attributes\n",
        "  that appears more than the treshold count\n",
        "  \"\"\"\n",
        "  true_positives = Counter()\n",
        "  false_positives = Counter()\n",
        "  false_negatives = Counter()\n",
        "  all_attributes = Counter()\n",
        "\n",
        "  for bio, gold_attributes in wiki_bio_test:\n",
        "    predicted_attributes = convert_to_dict(parse_bio(bio, finetuned_bio_parser_model))\n",
        "    update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes)\n",
        "\n",
        "  average_precision = 0\n",
        "  average_recall = 0\n",
        "  total = 0\n",
        "\n",
        "  for attribute in all_attributes:\n",
        "    if all_attributes[attribute] < threshold_count:\n",
        "      continue\n",
        "    print(attribute.upper())\n",
        "    try:\n",
        "      precision = true_positives[attribute] / (true_positives[attribute] + false_positives[attribute])\n",
        "    except:\n",
        "      precision = 0.0\n",
        "    try:\n",
        "      recall = true_positives[attribute] / (true_positives[attribute] + false_negatives[attribute])\n",
        "    except:\n",
        "      recall = 0.0\n",
        "    print(\"precision:\", precision)\n",
        "    print(\"recall:\", recall)\n",
        "    print(\"f-score:\", (precision+recall)/2)\n",
        "    print('---')\n",
        "    average_precision += precision\n",
        "    average_recall += recall\n",
        "    total += 1\n",
        "\n",
        "  print(\"AVERAGE\")\n",
        "  average_precision = average_precision/total\n",
        "  average_recall = average_recall/total\n",
        "  print(\"precision:\", average_precision)\n",
        "  print(\"recall:\", average_recall)\n",
        "  print(\"f-score:\", (average_precision+average_recall)/2)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "M5UXVTgaGHBD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to evaluate on the full test set, there are 237 test items.  You can set `max_test_items=237`.  Doing so will call your `parse_bio` function about 237 times, so be sure that you've got it properly debugged and working before running this code."
      ],
      "metadata": {
        "id": "fBCI95rQI8gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testset_filename='SynthBio_test.json'\n",
        "max_test_items=10\n",
        "wiki_bio_test = load_wiki_bio_test_set(testset_filename, max_test_items)\n",
        "evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5)"
      ],
      "metadata": {
        "id": "BDSuk0AWGlOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1237e93-147a-4eb8-e0c7-6df63b3f86fe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME\n",
            "precision: 0.4117647058823529\n",
            "recall: 0.7\n",
            "f-score: 0.5558823529411765\n",
            "---\n",
            "GENDER\n",
            "precision: 0.5\n",
            "recall: 1.0\n",
            "f-score: 0.75\n",
            "---\n",
            "NATIONALITY\n",
            "precision: 0.5\n",
            "recall: 1.0\n",
            "f-score: 0.75\n",
            "---\n",
            "BIRTH_DATE\n",
            "precision: 0.47368421052631576\n",
            "recall: 0.9\n",
            "f-score: 0.6868421052631579\n",
            "---\n",
            "BIRTH_PLACE\n",
            "precision: 0.6\n",
            "recall: 0.8823529411764706\n",
            "f-score: 0.7411764705882353\n",
            "---\n",
            "KNOWN_FOR\n",
            "precision: 0.5\n",
            "recall: 0.6666666666666666\n",
            "f-score: 0.5833333333333333\n",
            "---\n",
            "ALMA_MATER\n",
            "precision: 0.5714285714285714\n",
            "recall: 0.6153846153846154\n",
            "f-score: 0.5934065934065934\n",
            "---\n",
            "AWARDS\n",
            "precision: 0.45454545454545453\n",
            "recall: 0.5\n",
            "f-score: 0.4772727272727273\n",
            "---\n",
            "MOTHER\n",
            "precision: 0.4444444444444444\n",
            "recall: 0.8\n",
            "f-score: 0.6222222222222222\n",
            "---\n",
            "FATHER\n",
            "precision: 0.35714285714285715\n",
            "recall: 0.5555555555555556\n",
            "f-score: 0.4563492063492064\n",
            "---\n",
            "PARTNER\n",
            "precision: 0.4375\n",
            "recall: 0.875\n",
            "f-score: 0.65625\n",
            "---\n",
            "CHILDREN\n",
            "precision: 0.5238095238095238\n",
            "recall: 0.6470588235294118\n",
            "f-score: 0.5854341736694678\n",
            "---\n",
            "NOTABLE_TYPE\n",
            "precision: 0.5\n",
            "recall: 1.0\n",
            "f-score: 0.75\n",
            "---\n",
            "DEATH_DATE\n",
            "precision: 0.4444444444444444\n",
            "recall: 0.5714285714285714\n",
            "f-score: 0.5079365079365079\n",
            "---\n",
            "DEATH_PLACE\n",
            "precision: 0.6666666666666666\n",
            "recall: 1.0\n",
            "f-score: 0.8333333333333333\n",
            "---\n",
            "DEATH_CAUSE\n",
            "precision: 0.375\n",
            "recall: 0.6\n",
            "f-score: 0.4875\n",
            "---\n",
            "RESTING_PLACE\n",
            "precision: 0.4\n",
            "recall: 0.5\n",
            "f-score: 0.45\n",
            "---\n",
            "OCCUPATION\n",
            "precision: 0.25\n",
            "recall: 0.2857142857142857\n",
            "f-score: 0.26785714285714285\n",
            "---\n",
            "HOMETOWN\n",
            "precision: 0.5\n",
            "recall: 0.625\n",
            "f-score: 0.5625\n",
            "---\n",
            "CITIZENSHIP\n",
            "precision: 0.375\n",
            "recall: 0.75\n",
            "f-score: 0.5625\n",
            "---\n",
            "AVERAGE\n",
            "precision: 0.4642715439445316\n",
            "recall: 0.7237080729727788\n",
            "f-score: 0.5939898084586552\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well did your model perform?"
      ],
      "metadata": {
        "id": "rCMqucECM9EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - fill in these values\n",
        "average_precision = 0.4642715439445316\n",
        "average_recall = 0.7237080729727788\n",
        "average_fscore = 0.5939898084586552\n",
        "\n",
        "# What attributes had the highest F-scorre\n",
        "best_attributes = {\n",
        "    \"DEATH_PLACE\" : 0.8333333333333333,\n",
        "}\n",
        "\n",
        "# What attributes had the lowest F-scorre\n",
        "worst_attributes = {\n",
        "    \"OCCUPATION\" : 0.26785714285714285,\n",
        "}\n",
        "\n",
        "# What could you do the perform the model's performance?\n",
        "potential_improvements = \"\"\"\n",
        "In the few shot examples, we could include a step-by-step process extracting a\n",
        "person's occupation from their biography. This is likely because many bios\n",
        "identify a person with a few different nouns as to the roles they've filled\n",
        "throughout their lives (i.e. identifying Jill Biden as both an educator and\n",
        "the First Lady). So we could prep the model by going through each of those roles\n",
        "manually and being like \"is 'educator' an occupation?\", \"is 'First Lady' an\n",
        "occupation?\", \"therefore Jill Biden's occupation is 'educator'\". Something like\n",
        "that.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O5frXHaeNFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback questions"
      ],
      "metadata": {
        "id": "5NFilM6oNv-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many hours did you spend on this assignment? Just an approximation is fine.\n",
        "num_hours_spent = 6\n",
        "\n",
        "# What did you think?  This was the first time we tried this assignment\n",
        "# so you're feedback is valable.\n",
        "feedback = \"\"\"\n",
        "I thought it was interesting. It was a bit confusing trying to get it to work\n",
        "with Colab though -- I had to spend a lot of trial and error time figuring\n",
        "out that I couldn't use the latest openai version and then how to reset\n",
        "my notebook's kernel. I also ran into a weird bug where it wouldn't let me\n",
        "create an ada model because it said there was already a file with that name?\n",
        "It was weird.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "yPS7_smBN-2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}